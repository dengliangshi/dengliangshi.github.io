---
layout: post
title: Some Tips for Building Neural Network Languge Models
abstract: In this post, some tips for the implemetation details of neural network language models will summarized, and the advantages or limits of each solution will also be disscused.
---

### 1. Introduction
Numberous works about neural network language models (NNLMs) are arised, they the whole architecture of NNLMs or optimization techniques 

### 2. Initialization
The initialization of nerual network language models has a significant effect on the training, and the parameters of neural network language models needed to be initialized include feature vectors of words and weight matrixes of neural network. Each parameter is commonly initialized by a random number generated by a uniform distribution with specified lower and upper limits. One simple way is to set the lower and upper limits with fixed values, like -0.1 and 0.1. A more adaptive way is use the column size of weight matrix to initialize its parameters ([Glorot and Bengio, 2010](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)). Take $$$U\in\mathbb{R}^{n\times{m}$ as an example, the lower and upper limits will be $$-sqrt{\frac{1}{m}}$$ and $$sqrt{\frac{1}{m}}$$. The initialization of parameters of feature vectors and bias vectors still use fixed lower and upper limits. In this way, the lower and upper limits do not need to be tuned when the size of martrix or vectors is changed. 

### 3. Training Unit
As is well known, the goal of neural network language models is to learn the distribution function of the word sequence in a language, and the words from a data set are usually treated as a single and long sequence. When traing neural language models on training data set, the parameters are updated every certain number of words. For recurrent network language models or lstm ones, the error should be back-propagated. The computation is much expensive to back-propagated the error through all the whole sequence, therefore, the truncated bptt method is used, this is to back-propagate error through only a few previous steps at each updating. However, when the data set is dealt with as a set of individual sentences, it is feasible to back-propagate errors through a whole sentence without truncation although sometimes several sentence may be very long. It makes more sense to 

### 4. Input Level


### 5. Unknown Words
In 

### 6. Other
using one dimension arrays for weight matrix will be much faster than using two dimension arrays.