---
layout: post
title: Some Tips for Building Neural Network Languge Models
abstract: In this post, some tips for the implemetation details of neural network language models will summarized, and the advantages or limits of each solution will also be disscused.
---

### 1. Introduction
Numberous works about neural network language models (NNLMs) are arised, they the whole architecture of NNLMs or optimization techniques 

### 2. Initialization
The initialization of nerual network language models has a significant effect on the training, and the parameters of neural network language models needed to be initialized include feature vectors of words and weight matrixes of neural network. Each parameter is commonly initialized by a random number generated by a uniform distribution with specified lower and upper limits. One simple way is to set the lower and upper limits with fixed values, like -0.1 and 0.1. A more adaptive way is use the column size of weight matrix to initialize its parameters ([Glorot and Bengio, 2010](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)). Take $$U\in\mathbb{R}^{n\times{m}}$$ as an example, the lower and upper limits will be $$-\sqrt{1/m}$$ and $$\sqrt{1/m}$$. The initialization of parameters of feature vectors and bias vectors still use fixed lower and upper limits. In this way, the lower and upper limits do not need to be tuned when the size of martrix or vectors is changed. 

### 3. Training Unit
As is well known, the goal of neural network language models is to learn the distribution function of the word sequence in a language, and the words from a data set are usually treated as a single and long sequence. When traing neural language models on training data set, the parameters are updated every certain number of words. For recurrent network language models or lstm ones, the error should be back-propagated. The computation is much expensive to back-propagated the error through all the whole sequence, therefore, the truncated bptt method is used, this is to back-propagate error through only a few previous steps at each updating. However, when the data set is dealt with as a set of individual sentences, it is feasible to back-propagate errors through a whole sentence without truncation although sometimes several sentence may be very long. It makes more sense to take a sentence as a individual when dealing with a language.

### 4. Input Level
There are mainly two ways to define the minmum unit of a language , 

### 5. Unknown Words
Unknown words, which is also called words out of vocabulary, is an unavoidable problem when building a language model, because it is impossible to  include all words of a language while training. 

When neural network models are applied into language modeling, the computation is very expensive, in fact, it takes around one day to train nerual network language models even on a small corpus like Brown Corpus. One of the factors, which lead to the great computational burden of neural network language models, is the size of vocabulary. In order to reduce the calculated amount, not all the words from training set are added into pre-build vocabulary. The words out of vocabulary are all treated as unknown words and share one feature vector, but this will decrease the performance of neural network language, in another way, higher perplexity.

After some speep-up techniques are proposaled, the size of vocabulary is not a problem any more and all words from training set are included in vocabulary. There is no unknown words during training. However, some words in validation set or test set may be not in the vocabulary and this always happens. The common way to deal with this problem is to assign a feature vector, whose elements are all zero, to those words out of vocabulary, and do not take those words into account when count the total number of words and the probability of word sequence. Take a word sequence $$w_1w_2\dots{w_n}$$ as an example, there is only one word $$w_t$$ which is out of vocabulary. Before running nerual network language model on this word sequence, a start and end mark should be added and make them as $$w_0$$ and $$w_{n+1}$$ respectively. The inputs and outputs of neural network language model are as showed in Figure 3, and $$h_i (i=1, 2, \dots, n+1)$$ is the previous context of word $$w_i$$, i.e., $$h_i = w_0w_1\dots{w_{i-1}}$$.

<div style="text-align: center;">
<img src="/images/tips/unknownword.png">
<p>Figure 3. How to deal with unknown words</p>
</div>

When calculating the entropy or perplexity, the total number of words is $$n$$, because word $$w_t$$ and start mark are not counted. And the probabilty of this word sequence is:

$$P(w_1w_2\dots{w_n}) = $\prod_{i=1}^{t-1}P(w_i{\mid}h_i)\dot\prod_{j=t+1}^{n+1}P(w_j{\mid}h_j)$

the conditional probability of word $$w_t$$ is excluded.

### 6. Other
using one dimension arrays for weight matrix will be much faster than using two dimension arrays.